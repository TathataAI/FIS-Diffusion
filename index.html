<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Fuzzy Intent Strategy: Enhancing Intent Understanding for Ambiguous Prompts through Human-Machine Co-Adaptation</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>
  

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Fuzzy Intent Strategy: Enhancing Intent Understanding for Ambiguous Prompts through Human-Machine Co-Adaptation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Yangfan He</a>,</span>
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Jianhui Wang</a>,</span>
              <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Kun Li</a>,</span>
              <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Yijin Wang</a>,</span>
              <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Li Sun</a>,</span>
              <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Sida Li</a>,</span>
              <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Jun Yin</a>,</span>
              <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Miao Zhang</a><sup>†</sup>
              <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Xueqian Wang</a>,</span>
                  </div>



                    <!-- Dataset link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Datasets</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
            
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">University of Minnesota - Twin Cities, Tsinghua University</span>
                    <span class="eql-cntrb"><small><br><sup>†</sup>Indicates Corresponding Author</small></span>
                  </div>
            
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Modern image generation systems are capable of producing realistic and high-quality images. Nevertheless, user prompts often contain ambiguities that make it difficult for these systems to interpret the true intentions of their users accurately. Consequently, many users must modify their prompts several times to ensure the generated images meet their expectations. 
While some approaches aim to enhance prompts to meet user needs more accurately, the model still struggles to grasp the requirements of users without specialized expertise. 
We propose Visual Co-Adaptation (VCA), a framework that leverages a pre-trained language model fine-tuned via reinforcement learning to iteratively refine user prompts, aligning generated images with user preferences. At its core, the Incremental Context-Enhanced Dialogue Block employs multi-turn dialogues to disambiguate prompts through clarifying questions and user feedback. The Semantic Exploration and Disambiguation Module (SESD) integrates techniques like Retrieval-Augmented Generation (RAG) and CLIP-based scoring to resolve ambiguities in complex prompts. To ensure pixel-level precision and global consistency, the Pixel Precision and Consistency Optimization Module (PPCO) utilizes Proximal Policy Optimization (PPO) and attention mechanisms to fine-tune image details while maintaining visual harmony. A human-in-the-loop feedback mechanism further enhances model performance by integrating user feedback into the training loops of diffusion models. Extensive experiments show that VCA significantly improves user satisfaction, image-text alignment, and aesthetic quality. Compared to state-of-the-art systems like DALL-E 3, Stable Diffusion, and Imagen, our model reduces the average number of dialogue rounds to 4.3, achieves a CLIP score of 0.92, and increases user satisfaction to 4.73/5. Furthermore, we collected multi-round dialogue datasets including prompt and image pairs as well as user intent for various experiments designed to showcase its efficacy within our dataset.</p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Single image container -->
      <div class="item has-text-centered">
        <img src="ambiguious.jpg" alt="MY ALT TEXT" />
        <!-- Subtitle text container with centered text -->
        <p class="subtitle">
         Demonstration of disambiguation in text-to-image generation. Each row showcases examples of ambiguous prompts and their corresponding visual interpretations. For instance, the prompt "jam" results in both "fruit jam" and "traffic jam," while "bat" is interpreted as both a flying mammal and a baseball bat. Similarly, "spring" leads to "spring water" and "spring flower," and "mouse" generates both a "computer mouse" and an actual rodent. This highlights the importance of context and specificity in resolving ambiguities, illustrating the model's capability to handle polysemy effectively.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End Single Image Display -->
  
  
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Single image container -->
      <div class="item has-text-centered">
        <img src="imagereward2.jpg" alt="MY ALT TEXT" />
        <!-- Subtitle text container with centered text -->
        <p class="subtitle">
         Overview of the overall framework: (a) The LLM refines the user's prompt through Word Swap, Adding a Phrase, or Attention Re-weighting, using PPO until satisfaction; (b) Effects of these operations on the image are demonstrated; (c) PPO's internal loop computes rewards based on CLIP feedback to meet the threshold or limit.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End Single Image Display -->
  

<!-- Single Image Display -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Single image container -->
      <div class="item has-text-centered">
        <img src="flowchart.jpg" alt="MY ALT TEXT" />
        <!-- Subtitle text container with centered text -->
        <p class="subtitle">
            The flowchart of our framework, comprising a high-level Interpreter Agent and a low-level Controller Agent for text-to-image generation.
      </h2>
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End Single Image Display -->

<!-- Single Image Display -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Single image container -->
      <div class="item has-text-centered">
        <img src="clarify.jpg" alt="MY ALT TEXT" />
        <!-- Subtitle text container with centered text -->
        <p class="subtitle">
            Resolving Ambiguous Prompts: Addressing Insufficient Context and Semantic Uncertainty through Clarifying Questions and Visual Setups.</h2>
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End Single Image Display -->

<!-- Single Image Display -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Single image container -->
      <div class="item has-text-centered">
        <img src="Visualization.jpg" alt="MY ALT TEXT" />
        <!-- Subtitle text container with centered text -->
        <p class="subtitle">
            The figure illustrates the iterative process of refining a prompt through six rounds. In each round, modifications are made using different strategies: adding phrases, attention re-weighting, and word swap. Each step's impact on the description of the image is shown visually, along with the type of modification used.
        </h2>
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End Single Image Display -->



<!-- Single Image Display -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Single image container -->
      <div class="item has-text-centered">
        <img src="visualization_beans.jpg" alt="MY ALT TEXT" />
        <!-- Subtitle text container with centered text -->
        <p class="subtitle">
            The comparison showcases our model’s ability to precise control and efficiently align with user intentions, such as refining soup images by adding specific elements like croutons. By the third round, our model fully captures the user's intention, demonstrating minimal need for further actions.
        </h2>
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End Single Image Display -->
  
<!-- Single Image Display -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Single image container -->
      <div class="item has-text-centered">
        <img src="userstudy.jpg" alt="MY ALT TEXT" />
        <!-- Subtitle text container with centered text -->
        <p class="subtitle">
                   The chart shows user feedback on a model, highlighting mixed responses with positive feedback on image coherence and capturing intentions, but concerns over response time.
        </h2>
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End Single Image Display -->

  
<!-- Single Image Display -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Single image container -->
      <div class="item has-text-centered">
        <img src="strategy.jpg" alt="MY ALT TEXT" />
        <!-- Subtitle text container with centered text -->
        <p class="subtitle">
            The figure shows the various operations applied during different rounds.
        </h2>
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End Single Image Display -->

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
